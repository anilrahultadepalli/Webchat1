{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anilrahultadepalli/Webchat1/blob/main/web_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f7e487a-b29e-4a80-8fb8-46a3340cdb0b",
      "metadata": {
        "id": "5f7e487a-b29e-4a80-8fb8-46a3340cdb0b",
        "outputId": "084917c9-ebcd-47d2-92e6-39a69e77370b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import json\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c210a6a2-0fde-4781-8fbf-45e1621bcc50",
      "metadata": {
        "id": "c210a6a2-0fde-4781-8fbf-45e1621bcc50"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.uchicago.edu/\"\n",
        "# url = \"https://www.javatpoint.com/dynamic-programming\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "013dfcda-f70e-4b13-9dce-4c5cdd5465ac",
      "metadata": {
        "id": "013dfcda-f70e-4b13-9dce-4c5cdd5465ac"
      },
      "source": [
        "### Fuction to Scrape data from URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2db43cc4-8afe-47a5-b193-76b888e0fe1f",
      "metadata": {
        "id": "2db43cc4-8afe-47a5-b193-76b888e0fe1f"
      },
      "outputs": [],
      "source": [
        "def scrapeData(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    print(f\"Fetching {url} - Status Code: {response.status_code}\")\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        return {\n",
        "            \"url\": url,\n",
        "            \"error\": f\"Failed to fetch content, status code: {response.status_code}\"\n",
        "        }\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Initialize structure for extracted content\n",
        "    structured_data = {\n",
        "        \"url\": url,\n",
        "        \"headings\": [],\n",
        "        \"paragraphs\": [],\n",
        "        \"other_text\": []\n",
        "    }\n",
        "\n",
        "    # Extracting headings\n",
        "    for heading in range(1, 7):\n",
        "        for tag in soup.find_all(f\"h{heading}\"):\n",
        "            text = tag.get_text(strip=True)\n",
        "            if text:\n",
        "                structured_data[\"headings\"].append(text)\n",
        "\n",
        "    # Extracting paragraphs\n",
        "    for p in soup.find_all(\"p\"):\n",
        "        text = p.get_text(strip=True)\n",
        "        if text:\n",
        "            structured_data[\"paragraphs\"].append(text)\n",
        "\n",
        "    # Extract other text content (excluding h1-h6 and p)\n",
        "    # for tag in soup.find_all(True):\n",
        "    #     if tag.name not in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\"]:\n",
        "    #         text = tag.get_text(strip=True)\n",
        "    #         if text:\n",
        "    #             structured_data[\"other_text\"].append(text)\n",
        "\n",
        "    # Remove duplicates while maintaining order\n",
        "    structured_data[\"headings\"] = list(dict.fromkeys(structured_data[\"headings\"]))\n",
        "    structured_data[\"paragraphs\"] = list(dict.fromkeys(structured_data[\"paragraphs\"]))\n",
        "    # structured_data[\"other_text\"] = list(dict.fromkeys(structured_data[\"other_text\"]))\n",
        "\n",
        "    return structured_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad4e7335-6310-42a4-b0f8-45522aa490a6",
      "metadata": {
        "id": "ad4e7335-6310-42a4-b0f8-45522aa490a6"
      },
      "source": [
        "### Saving Extracted data into json format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "725a40b2-cd25-4b96-89e1-9976625fc05f",
      "metadata": {
        "id": "725a40b2-cd25-4b96-89e1-9976625fc05f"
      },
      "outputs": [],
      "source": [
        "def save_to_json(data, output_dir):\n",
        "    # Extract domain name for filename\n",
        "    domain = data[\"url\"].split(\"//\")[-1].split(\"/\")[0]\n",
        "    file_name = f\"{domain}.json\"\n",
        "    file_path = os.path.join(output_dir, file_name)\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Data saved to {file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af691302-9227-482c-9584-7364d2e90007",
      "metadata": {
        "id": "af691302-9227-482c-9584-7364d2e90007",
        "outputId": "fcd58475-1f3d-40b4-b93c-51f46e98d7eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/saiteja/Programming/url-chat-bot'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d4b942b-6e64-4d25-8379-41a9d126088a",
      "metadata": {
        "id": "8d4b942b-6e64-4d25-8379-41a9d126088a",
        "outputId": "fcfee64b-f9b7-4a06-d725-c03873a71827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mchroma_db\u001b[m\u001b[m/      \u001b[34mscraped_data\u001b[m\u001b[m/   web-chat.ipynb\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65178e78-cf3b-4556-aa01-af2d93f88826",
      "metadata": {
        "id": "65178e78-cf3b-4556-aa01-af2d93f88826",
        "outputId": "aaa48179-6aea-4f04-f368-c847e5c4e12b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching https://www.uchicago.edu/ - Status Code: 200\n",
            "{\n",
            "    \"url\": \"https://www.uchicago.edu/\",\n",
            "    \"headings\": [\n",
            "        \"The Day Tomorrow Began\",\n",
            "        \"Go 'Inside the Lab' at UChicago\",\n",
            "        \"LATEST NEWS\",\n",
            "        \"We value rigorous inquiry\",\n",
            "        \"We foster independent thinking\",\n",
            "        \"Transformative education\",\n",
            "        \"Field-defining research\",\n",
            "        \"We advance ideas and humanity\",\n",
            "        \"Intellectual freedom\",\n",
            "        \"Community impact\",\n",
            "        \"Global impact\",\n",
            "        \"We call Chicago home\"\n",
            "    ],\n",
            "    \"paragraphs\": [\n",
            "        \"A diversity of people and ideas, coupled with free and open discourse, lays the foundation for students and scholars to bring forth original ideas that define fields and enrich human life.\",\n",
            "        \"UChicago students develop the habits of mind and intellectual skills needed to confront complex challenges.\",\n",
            "        \"UChicago researchers have contributed to some of the world\\u2019s greatest discoveries, advancements, and bodies of knowledge.\",\n",
            "        \"Faculty have a free and challenging environment in which to pursue the most original research.\",\n",
            "        \"As a community partner, we invest in Chicago\\u2019s South Side across such areas as health, education,  economic growth, and the arts.\",\n",
            "        \"We are an international community of scholars working to solve the world's most pressing issues, with initiatives and programs on all seven continents.\",\n",
            "        \"Chicago is not only in our name, it\\u2019s woven into the fabric of this institution. Located in the Hyde Park neighborhood, we benefit from the diversity, arts, and vibrant culture of our South Side community.\"\n",
            "    ],\n",
            "    \"other_text\": []\n",
            "}\n",
            "Data saved to ./scraped_data/www.uchicago.edu.json\n"
          ]
        }
      ],
      "source": [
        "output_directory = \"./scraped_data\"\n",
        "\n",
        "scraped_data = scrapeData(url)\n",
        "print(json.dumps(scraped_data, indent=4))\n",
        "data = scraped_data\n",
        "\n",
        "if \"error\" not in scraped_data:\n",
        "    save_to_json(scraped_data, output_directory)\n",
        "else:\n",
        "    print(scraped_data[\"error\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3713dfa7-3138-432c-a5e3-371367a3b6a7",
      "metadata": {
        "id": "3713dfa7-3138-432c-a5e3-371367a3b6a7",
        "outputId": "9015048c-7bcc-421a-9cf7-260070244090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain pinecone-client sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c00950f4-29cd-4bb1-9c19-208bf6212844",
      "metadata": {
        "id": "c00950f4-29cd-4bb1-9c19-208bf6212844"
      },
      "source": [
        "### Chunking Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5cabb3-f9d9-4635-9691-b1a0d43972b5",
      "metadata": {
        "id": "4c5cabb3-f9d9-4635-9691-b1a0d43972b5"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "def chunk_data(data, chunk_size=256):\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
        "\n",
        "    combined_text = []\n",
        "    if 'headings' in data:\n",
        "        combined_text.extend(data['headings'])\n",
        "    if 'paragraphs' in data:\n",
        "        combined_text.extend(data['paragraphs'])\n",
        "    if 'other_text' in data:\n",
        "        combined_text.extend(data['other_text'])\n",
        "\n",
        "    full_text = \"\\n\\n\".join(combined_text)\n",
        "    chunks = text_splitter.split_text(full_text)\n",
        "\n",
        "    # Wrap chunks in Document objects\n",
        "    document_chunks = [Document(page_content=chunk) for chunk in chunks]\n",
        "\n",
        "    return document_chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06c73bfa-9517-475d-8eb2-cdb91610eb0e",
      "metadata": {
        "id": "06c73bfa-9517-475d-8eb2-cdb91610eb0e"
      },
      "source": [
        "### Embedding and Uploading to a Vector Database (Pinecone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba59e90-e603-44e6-8599-4b5eec0e94e5",
      "metadata": {
        "id": "bba59e90-e603-44e6-8599-4b5eec0e94e5"
      },
      "outputs": [],
      "source": [
        "def insert_or_fetch_embeddings(index_name, chunks):\n",
        "    # importing the necessary libraries and initializing the Pinecone client\n",
        "    import pinecone\n",
        "    from langchain_community.vectorstores import Pinecone\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "    from pinecone import ServerlessSpec\n",
        "\n",
        "\n",
        "    pc = pinecone.Pinecone()\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  # 512 works as well\n",
        "\n",
        "    # loading from existing index\n",
        "    if index_name in pc.list_indexes().names():\n",
        "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
        "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
        "        print('Ok')\n",
        "    else:\n",
        "        # creating the index and embedding the chunks into the index\n",
        "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
        "\n",
        "        # creating a new index\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=1536,\n",
        "            metric='cosine',\n",
        "            spec=ServerlessSpec(\n",
        "                cloud=\"aws\",\n",
        "                region=\"us-east-1\"\n",
        "        )\n",
        "        )\n",
        "\n",
        "        # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
        "        # inserting the embeddings into the index and returning a new Pinecone vector store object.\n",
        "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
        "        print('Ok')\n",
        "\n",
        "    return vector_store\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad356fc-2511-44c6-964e-8f51c9295535",
      "metadata": {
        "id": "0ad356fc-2511-44c6-964e-8f51c9295535"
      },
      "outputs": [],
      "source": [
        "def delete_pinecone_index(index_name='all'):\n",
        "    import pinecone\n",
        "    pc = pinecone.Pinecone()\n",
        "\n",
        "    if index_name == 'all':\n",
        "        indexes = pc.list_indexes().names()\n",
        "        print('Deleting all indexes ... ')\n",
        "        for index in indexes:\n",
        "            pc.delete_index(index)\n",
        "        print('Ok')\n",
        "    else:\n",
        "        print(f'Deleting index {index_name} ...', end='')\n",
        "        pc.delete_index(index_name)\n",
        "        print('Ok')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf57a5f7-6b5f-43ab-b6c1-e21d52a43efc",
      "metadata": {
        "id": "bf57a5f7-6b5f-43ab-b6c1-e21d52a43efc"
      },
      "source": [
        "## Asking and Getting Answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93f8d8d2-9809-4365-a34d-5c68033770eb",
      "metadata": {
        "id": "93f8d8d2-9809-4365-a34d-5c68033770eb"
      },
      "outputs": [],
      "source": [
        "def ask_and_get_answer(vector_store, q, k=3):\n",
        "    from langchain.chains import RetrievalQA\n",
        "    from langchain_openai import ChatOpenAI\n",
        "\n",
        "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
        "\n",
        "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
        "\n",
        "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
        "\n",
        "    answer = chain.invoke(q)\n",
        "    return answer\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32a2b7cd-142c-448e-af49-f6dff8be6e74",
      "metadata": {
        "id": "32a2b7cd-142c-448e-af49-f6dff8be6e74"
      },
      "source": [
        "## Using Chroma as a Vector DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9763b967-88cd-4c6a-a929-1572d42e661e",
      "metadata": {
        "id": "9763b967-88cd-4c6a-a929-1572d42e661e",
        "outputId": "acc1577c-a376-4bf2-c953-e6b9e57a9ae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.47.0 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -q chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa936a3a-a416-4f24-a140-4500166b004b",
      "metadata": {
        "id": "fa936a3a-a416-4f24-a140-4500166b004b"
      },
      "outputs": [],
      "source": [
        "def create_embeddings_chroma(chunks, persist_directory='./chroma_db'):\n",
        "    from langchain.vectorstores import Chroma\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "    # Instantiate an embedding model from OpenAI (smaller version for efficiency)\n",
        "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
        "\n",
        "    # Create a Chroma vector store using the provided text chunks and embedding model,\n",
        "    # configuring it to save data to the specified directory\n",
        "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)\n",
        "\n",
        "    return vector_store  # Return the created vector store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1677d40-063b-447d-978a-4a4e996cf58c",
      "metadata": {
        "id": "c1677d40-063b-447d-978a-4a4e996cf58c"
      },
      "outputs": [],
      "source": [
        "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
        "    from langchain.vectorstores import Chroma\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "    # Instantiate the same embedding model used during creation\n",
        "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
        "\n",
        "    # Load a Chroma vector store from the specified directory, using the provided embedding function\n",
        "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "    return vector_store  # Return the loaded vector store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d5c4811-fceb-4950-92b6-0d411fb82e55",
      "metadata": {
        "id": "7d5c4811-fceb-4950-92b6-0d411fb82e55",
        "outputId": "2dc4a62f-bf6d-42a4-ab8a-d6718055523d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -U langchain-community -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a64a6dca-e0d4-4246-a074-3b8de8692d3f",
      "metadata": {
        "id": "a64a6dca-e0d4-4246-a074-3b8de8692d3f",
        "outputId": "2c37f91a-4866-417d-8433-491f9dfec259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -q langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81586f12-1071-414b-97b2-eec205dae3e2",
      "metadata": {
        "id": "81586f12-1071-414b-97b2-eec205dae3e2"
      },
      "outputs": [],
      "source": [
        "# Splitting the document into chunks\n",
        "chunks = chunk_data(data, chunk_size=256)\n",
        "\n",
        "# Creating a Chroma vector store using the provided text chunks and embedding model (default is text-embedding-3-small)\n",
        "vector_store = create_embeddings_chroma(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e3896e2-4a5f-44d5-8e45-1dd77ebc8b58",
      "metadata": {
        "id": "9e3896e2-4a5f-44d5-8e45-1dd77ebc8b58"
      },
      "source": [
        "## Adding Memory (Chat History)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1402cf38-b65c-4a91-a73d-c778dc21ff84",
      "metadata": {
        "id": "1402cf38-b65c-4a91-a73d-c778dc21ff84"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain  # Import class for building conversational AI chains\n",
        "from langchain.memory import ConversationBufferMemory  # Import memory for storing conversation history\n",
        "\n",
        "# Instantiate a ChatGPT LLM (temperature controls randomness)\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
        "\n",
        "# Configure vector store to act as a retriever (finding similar items, returning top 5)\n",
        "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
        "\n",
        "\n",
        "# Create a memory buffer to track the conversation\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "crc = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,  # Link the ChatGPT LLM\n",
        "    retriever=retriever,  # Link the vector store based retriever\n",
        "    memory=memory,  # Link the conversation memory\n",
        "    chain_type='stuff',  # Specify the chain type\n",
        "    verbose=False  # Set to True to enable verbose logging for debugging\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f96f3b83-19de-492f-b84f-43815fe9d674",
      "metadata": {
        "id": "f96f3b83-19de-492f-b84f-43815fe9d674"
      },
      "outputs": [],
      "source": [
        "# create a function to ask questions\n",
        "def ask_question(q, chain):\n",
        "    result = chain.invoke({'question': q})\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bfe0048-37b8-465b-8043-648a8e9fa78d",
      "metadata": {
        "id": "4bfe0048-37b8-465b-8043-648a8e9fa78d"
      },
      "outputs": [],
      "source": [
        "chunks = chunk_data(data, chunk_size=256)\n",
        "vector_store = create_embeddings_chroma(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d03da93f-6153-4a5b-9f82-4855cc3c20aa",
      "metadata": {
        "id": "d03da93f-6153-4a5b-9f82-4855cc3c20aa"
      },
      "source": [
        "### Loop for asking questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "210a0f46-7e85-42ea-9a63-8594b763700a",
      "metadata": {
        "id": "210a0f46-7e85-42ea-9a63-8594b763700a",
        "outputId": "dc556476-a0ea-46ea-bd08-4da7d269f469"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Your question:  Give a summary of the data\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The data involves solving problems using a tabulation technique instead of recursion to avoid stack overflow issues and overhead. Results are stored in a matrix to keep track of intermediate values. An example is given with an array containing 0 and 1 values at specific positions.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Your question:  What are the key take aways\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The key takeaways from using a tabulation technique instead of recursion to avoid stack overflow issues and overhead are:\n",
            "1. By using tabulation, we solve problems iteratively and store results in a matrix.\n",
            "2. This approach eliminates the need for recursion, thus avoiding stack overflow issues.\n",
            "3. Storing intermediate results in a matrix allows for efficient reuse of values and optimization of the solution.\n",
            "\n",
            "An example involving an array with 0 and 1 values at specific positions could be a problem where we need to find the maximum sum of non-adjacent elements in an array. By using tabulation, we can create a matrix to store the maximum sum at each index, considering whether to include the current element or skip it based on the condition of not selecting adjacent elements. This approach efficiently solves the problem without the overhead of recursion.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Your question:  quit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bye bye!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    q = input('Your question: ')\n",
        "    if q.lower() in 'exit quit bye':\n",
        "        print('Bye bye!')\n",
        "        break\n",
        "    result = ask_question(q, crc)\n",
        "    print(result['answer'])\n",
        "    print('-' * 100)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b03e0f-48d1-4560-aa1a-f83e06b610c7",
      "metadata": {
        "id": "11b03e0f-48d1-4560-aa1a-f83e06b610c7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}